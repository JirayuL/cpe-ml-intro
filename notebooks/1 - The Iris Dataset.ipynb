{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your first Machine Learning Model\n",
    "\n",
    "As we've mentioned in the slide, we're going to start making your first Machine Learning model *from scratch*!\n",
    "\n",
    "For revision, Machine Learning model consists of two parts*: __data__ and __method (algorithm)__. We'll start with the data part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the Iris dataset\n",
    "\n",
    "![imgs/iris_petal_sepal.png](imgs/iris_petal_sepal.png)\n",
    "\n",
    "We'll first play around with the [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set), a dataset consisting of lengths and widths on the petal and sepal of different Iris flower.\n",
    "\n",
    "We'll first load our data with a tool called __Pandas__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset file\n",
    "names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'label']\n",
    "iris_data = pandas.read_csv('datasets/iris.data', names=names)\n",
    "\n",
    "# Create a colour map column\n",
    "classes = list(iris_data['label'].unique())\n",
    "iris_data['colour'] = [classes.index(i) for i in iris_data['label']]\n",
    "\n",
    "# Trim only first 10 rows of the data.\n",
    "iris_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = scatter_matrix(iris_data, c=iris_data['colour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(iris_data['petal_length'], iris_data['petal_width'], c=iris_data['colour'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can simply get $n^{th}$ row of data by using `iris_data.loc[n]`, and get the property of any row with the string index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.loc[0]['sepal_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! This is how you'll basically the data in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heading over to the model\n",
    "\n",
    "As we've already \"get\" the concept of what we're trying to achieve with the $k$-NN algorithm, when given the petal/sepal length/width of the flower we wish to \"predict\", we simply calculate the euclidian distant from the desired length/widths given\n",
    "\n",
    "We can do this by our hand, but fortunately, Python's got a machine learning library called `sklearn`, of which we can import and use it simply.\n",
    "\n",
    "Head to the [sklearn's documentation on $k$-nearest neighbour](#), and answer the following question.\n",
    "\n",
    "* How should the model be imported?\n",
    "* How can we create the model's instance?\n",
    "* How can we train the model with our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris_data.loc[:, 'sepal_length':'petal_width']\n",
    "y = iris_data.loc[:, 'colour']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model, and train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "clf = KNeighborsClassifier(k)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to predict one data point around here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [4, 1.5, 6, 1.5]\n",
    "\n",
    "clf.predict([preds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see the decision region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "for i, n in enumerate([[0, 1, 2, 3], [2, 3, 0, 1], [0, 2, 1, 3], [1, 3, 0, 2]]):\n",
    "    plot_decision_regions(X.values, y.values, clf=clf,\n",
    "                         filler_feature_values={n[2]: preds[n[2]], n[3]: preds[n[3]]},\n",
    "                         filler_feature_ranges={n[2]: 2, n[3]: 2},\n",
    "                         feature_index=[n[0], n[1]],\n",
    "                         ax=ax.flat[i])\n",
    "    ax.flat[i].scatter([preds[n[0]]], [preds[n[1]]], c='red', s=60, marker='X')\n",
    "    ax.flat[i].set_xlabel(names[n[0]])\n",
    "    ax.flat[i].set_ylabel(names[n[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the $k$ for the best accuracy\n",
    "\n",
    "__Question__: Why does $k = 1$ gives us the best accuracy (of 1.0 -- meaning that the model can predict all the data labels correctly)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing-Training split\n",
    "\n",
    "The \"good\" intuition for measuring the accuracy is that we should split our dataset into testing set and training set.\n",
    "\n",
    "`sklearn`'s got a function for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size = 0.3, random_state = 4)\n",
    "(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_2 = 1\n",
    "clf_tt = KNeighborsClassifier(k_2)\n",
    "clf_tt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 51):\n",
    "    clf_tt = KNeighborsClassifier(k)\n",
    "    clf_tt.fit(X_train, y_train)\n",
    "    score = clf_tt.score(X_test, y_test)\n",
    "    print(\"k = {}\\tscore = {}\".format(k, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
